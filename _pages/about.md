---
permalink: /
title: "About me"
layout: single
---

Hi!, I'm Wei Jie Yeo, currently a PhD student at Nanyang Technological University, Singapore, supervised by Prof. Erik Cambria. My main research interests lies between the intersection of NLP and Interpretability, with a focus on improving the current lack of understanding in how AI systems model various complex behaviors. Lately, I have been deeply interested in finding ways to utilize interpretability to improve problems in AI safety, such as jailbreak or prompt injection attacks.

## Employment
My expected graduation is somewhere towards the end of this year and will be actively seeking a full-time position in the industrial sector. Feel free to connect if you think I am a suitable candidate! [CV](/files/Resume.pdf)

## Publications
Understanding Refusal in Language Models with Sparse Autoencoders
**Wei Jie Yeo**, Nirmalendu Prakash, Clement Neo, Roy Ka-Wei Lee, Erik Cambria, Ranjan Satapathy

*Preprint*, 2025.
[[Paper]](https://arxiv.org/abs/2505.23556) [[Code]](https://github.com/wj210/refusal_sae)

Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads
**Wei Jie Yeo**, Rui Mao, Moloud Abdar, Erik Cambria, Ranjan Satapathy

*Preprint*, 2025.
[[Paper]](https://www.arxiv.org/abs/2505.17425) [[Code]](https://github.com/wj210/CLIP_LTC)

A comprehensive review on financial explainable AI
**Wei Jie Yeo**, Wihan Van Der Heever, Rui Mao, Erik Cambria, Ranjan Satapathy, Gianmarco Mengaldo
*AIRE Journal*, 2025.
[[Paper]](https://link.springer.com/article/10.1007/s10462-024-11077-7)

Self-training Large Language Models through Knowledge Detection
**Wei Jie Yeo**, Teddy Ferdinan, Przemyslaw Kazienko, Ranjan Satapathy, Erik Cambria
*EMNLP*, 2024.
[[Paper]](https://aclanthology.org/2024.findings-emnlp.883/) [[Code]](https://github.com/wj210/Self-Training-LLM)

How Interpretable are Reasoning Explanations from Prompting Large Language Models?  
**Wei Jie Yeo**, Ranjan Satapathy, Goh Siow Mong, Erik Cambria
*NAACL*, 2024.  
[[Paper]](https://aclanthology.org/2024.findings-naacl.138/) [[Code]](https://github.com/wj210/CoT_interpretability)

Plausible Extractive Rationalization through Semi-Supervised Entailment Signal
**Wei Jie Yeo**, Ranjan Satapathy, Erik Cambria
*ACL*, 2024
[[Paper]](https://aclanthology.org/2024.findings-acl.307/) [[Code]](https://github.com/wj210/NLI_ETP)



