---
permalink: /
title: "About me"
layout: single
---

Hi!, I'm Wei Jie Yeo, currently a PhD student at Nanyang Technological University, Singapore, supervised by Prof. Erik Cambria. My main research interests lies between the intersection of NLP and Interpretability, with a focus on improving the current lack of understanding in how AI systems model various complex behaviors. Lately, I have been deeply interested in finding ways to utilize interpretability to improve problems in AI safety, such as jailbreak or prompt injection attacks.

## Employment
My expected graduation is towards the end of 2025 and I will be actively seeking a full-time position in the industry. Feel free to connect if you think I am a suitable candidate! [CV](/files/Resume.pdf)

## Selected Publications
Understanding Refusal in Language Models with Sparse Autoencoders<br>
**Wei Jie Yeo**, Nirmalendu Prakash, Clement Neo, Roy Ka-Wei Lee, Erik Cambria, Ranjan Satapathy<br>
*Preprint*, 2025.<br>
[[Paper]](https://arxiv.org/abs/2505.23556) [[Code]](https://github.com/wj210/refusal_sae)

Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads<br> 
**Wei Jie Yeo**, Rui Mao, Moloud Abdar, Erik Cambria, Ranjan Satapathy<br> 
*Preprint*, 2025.<br> 
[[Paper]](https://www.arxiv.org/abs/2505.17425) [[Code]](https://github.com/wj210/CLIP_LTC)

A comprehensive review on financial explainable AI<br> 
**Wei Jie Yeo**, Wihan Van Der Heever, Rui Mao, Erik Cambria, Ranjan Satapathy, Gianmarco Mengaldo<br> 
*AIRE Journal*, 2025.<br> 
[[Paper]](https://link.springer.com/article/10.1007/s10462-024-11077-7)

Self-training Large Language Models through Knowledge Detection<br>
**Wei Jie Yeo**, Teddy Ferdinan, Przemyslaw Kazienko, Ranjan Satapathy, Erik Cambria<br> 
*EMNLP*, 2024. <br>
[[Paper]](https://aclanthology.org/2024.findings-emnlp.883/) [[Code]](https://github.com/wj210/Self-Training-LLM)

How Interpretable are Reasoning Explanations from Prompting Large Language Models?<br>
**Wei Jie Yeo**, Ranjan Satapathy, Goh Siow Mong, Erik Cambria<br> 
*NAACL*, 2024.<br>
[[Paper]](https://aclanthology.org/2024.findings-naacl.138/) [[Code]](https://github.com/wj210/CoT_interpretability)

Plausible Extractive Rationalization through Semi-Supervised Entailment Signal<br> 
**Wei Jie Yeo**, Ranjan Satapathy, Erik Cambria<br> 
*ACL*, 2024<br> 
[[Paper]](https://aclanthology.org/2024.findings-acl.307/) [[Code]](https://github.com/wj210/NLI_ETP)



